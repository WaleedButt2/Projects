{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')\n",
    "labels=df.pop('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN1ElEQVR4nO3dbcz9BV3H8c8v/gqaCGIOAUFuBLwPpaFkkigYrhvSjGUiaCjolDJEk2W5kqREpBI2IVPDWN5stkoTAdNIU8qshJSbRBIDccI0h2Fmvx5c9X16HbfPJg9er8dnn3P+13XO9d558N93Wdd1DQAk+b7v9QsA4J5DFAAYogDAEAUAhigAMEQBgCEKAAxRAGDs2PSBy7JUnnB974MqO0my/OxbO0MfOL6zkyTXd/4v4HrGRZWdJFnuOLmy8y8PfH9lJ0nOXp9d2fmTMzrvyyR5xfmdnfPO2vhjta0bzrlvZeeoo8+q7CTJl59T2jq1+P9m1877YI/f672mO17eem9eVdpJsj5524f4pgDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDA2PhE1IVr5yLR8uLepaysnYtppaNNSZK/T2dseUFlJkmyPvDsys7y1S9VdrbGOu+nfSsrW857Q2noyv8uDSWHfKnzRvjgzqdVdpLkGQ96dWVnfVTvg7e88vmVnTvfuEdlJ0kOySmVnRt3PqqykyRrtv/c+aYAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAMbG5zhfekzpFOOVV3V2krxm6Zype9IGJ+o29fF9OicGd357ZSZJsuSWztAv9M4nrusPV3ZOe0ZlJkny1gfcv7LzPy/9emUnSfKQzufluJ16ZyaTzu/uedceXdlJkrztHaWh95Z2khtL5zjzrTd1djbkmwIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAY1nXdbOzY7WjW3/UGsqSkys7O3r/uHy7dcSt95KytF7T3r0LdRfcdmNl52Vr5z2QJOvyicrOW953dWUnSV5yvyM6Q0+/vbOT5M3HPbiyc/plp1Z2tlxcWem9w5MlX6jsPG7tnRf8dD637WN8UwBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIA47s4x1m6D9m8d3d5Z2Z5QmcnSU7c/dLKzmH5ZmUnSS7Piyo7H8o5lZ0tZ3VmNnz7bmJZPtwZuuSYzk6S153U2flOZyZJ8trXdXb2/LXe7+45OaGyc8T63spOkjz3l0tD7yztJFm/uv3P3DcFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAMbGl9eWdC6vrfnbyk6SLLl3aenw0k5yaelC3WWVlS33urmz87b9OztJahf41uUvOkNJlv2eWtlZH/X9lZ0kefiFT67sXH/A/pWdJMlSOgV2+I93dpKs//CByk7pvmTZqbWldb1o28f4pgDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDAEAUAxo5NH/iuR95ZecK3f/YBlZ0kyUWl43kv7swkyXOv/o/O0BH37+wkydK5fbk+q3es8LTSa2qdiU2Sk7/Y2Vke09lJkhx4dGXmjPUnKztJ8qaUznGWTmg2rV/8Qm1rOfvmztDFnffAFuc4AfguiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgLOu6bnYCa3l65QnvlcsrO0ny+dLVrYemcwUsSfKK0iWw8zozW0o/87XzHkiSHNr5me96Q+/yWulmXvY9vTSU5Etvbi313uPHlz53136x95o+v29n5zFL7/10zWWdf99ex/Ve060b/Ln3TQGAIQoADFEAYIgCAEMUABiiAMAQBQCGKAAwRAGAIQoADFEAYIgCAEMUABiiAMAQBQCGKAAwRAGAIQoAjB2bPnA58IrKEz7jpnMrO0ny0FzdGfrTMzo7SR7+00dWdq4779DKTpLcuR5b2dljeVZlZ0vnxOBV+fXKTpIcn29Xdm45+MGVnSR5Q36psnNT9q/sJMlFD+rs7L9f78xk69roZzozSZLbjuvs7N2Z2ZhvCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgCMjS+v5abOE5546QM7Q0k++Nx9OkPPvLSzk+Rzub2ys+TjlZ0k2WMpXbj6/c5Mkjxy186prMe9oHe967GlneUXS0NJ3lPaOW29ubSUvOUrnZ1lz9LnN0mOf3dlZlmfXdlJkj8ofe7WHFHZ2ZRvCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgLOu6bnQXcUnp7OHSOcPY9LwcVNu65Emdu6XLxyozW3Yv7Xy997vbrfR+evCxlZkkyfVXlIbO/nppKFlP3K2y88/P+3ZlJ0kOu+q4ys66XFnZSZJl7byfvlP8+3TVIzqv6eg9KjNJkvVj2//7fFMAYIgCAEMUABiiAMAQBQCGKAAwRAGAIQoADFEAYIgCAEMUABiiAMAQBQCGKAAwRAGAIQoADFEAYOzY9IF3LE+uPOG5h11Y2UmS337Vyyo7l5xUmdny8c7MnxUvQF3wjc4FqFfuenllJ0laB9OWX934Lbyt9YWl62QnlK4UJlmOeXtl5/S/uVdlJ0neX9pZXtj7ObXslMN7Y589tbOz3LuzsyHfFAAYogDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDAWNZ13eju4/LUzum8T3/krys7SfL4fLOy8+rcp7KTJJ96ylMqO1d+tDKTJNm9tHNNaSdJ9q1dG+2dLX300nmPf7j4mj63dl7TU+55ly+bv7ocubyjsvOJ259f2UmS7HltZWZdH13Z+b+1bR/hmwIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAY8fGj9y584SPz492hpLckr0rOw/JrZWdJFk+2lrqnaX62n3fXdnZ966fq+wkybtKl8DuSu+k2B2lnT2f3XtNN/xAa6n3fnpEaeeTJ/Z+Tqfs9/zO0MWdmSTJ2rkyuSy9a5Wb3Nn0TQGAIQoADFEAYIgCAEMUABiiAMAQBQCGKAAwRAGAIQoADFEAYIgCAEMUABiiAMAQBQCGKAAwRAGAIQoAjGVdNznQlizL/p1nfGnrmF9y0oWXVXY+WlnZ8m+7dM4eLnf3ThXmlNLOH5Z2kqwHln5O+32rspMkb1t+vrLzjx95X2UnSV5y5j9Vdu4697DKTpIcXHpr3tyZSZKce+eXKztP3eOnKjtJcspj/64zdF3vlOq6wcfFNwUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDAEAUAxsaX17LfuZ1nvOBVnZ0ky92lod1LO0lyQOlK0sHXdHaS5JuPrcysB1ZmkiTL7XdVdnZaD63sJMnXlt0qO7vm2srOls6Zs1OO/dfKTpLc74qHVXZ+N/ep7CRJ1v+szCy9w2vJn3+nMrN+cqfKTpLkidv/ffJNAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgCMzc9xls4C3hMtyz7FtX/vzGz6a9lI53d3+m2VmSTJi/bq/Ps+uPTel7/yxtLQy0o7Se7eubOzSy7pDCVZLzmpM1SaSYp/nZp/5kof4Yctn+kMJblxfcy2j/FNAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYCxY/OH/nHlCf9qObKykyRP+8xBpaXStbQkyV92Zs44v7OT5JyjOjtnPbN3De7NV3dOXP3MKZWZLWd2Zl5+Zu/ndM1tP9EZ2qt35qx1nOydJxevC57ZeVWHt86lJbmw9JO66wefVtnZ8pVtH+GbAgBDFAAYogDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYogDAEAUAhigAMEQBgCEKAAxRAGCIAgBDFAAYy7quxZt42/ut1i2/JHeWds5bey9q+aHS0KdKO0nW5RuVnSW7VnaS5ITSzkG5d2kpOWeX/+oM/VhnJkkuuLSzc8D9OjtJslfpZOXjq395Op/hU5fe++nifKszVPz7tG7wu/NNAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYCxY9MHnnH/iytPeP56amUnSfI7Z1VmnvihQyo7SZInXN/ZWXbp7CS5cO/SxbRbD+jsJHlPzi8tHV/aSdZXdS5cfeI3KzNJkiN3+43KzpLXVnb+f63h9cuhlZ0k+ZFXXFHZOeq6Yys7SXLlwzs7Dz27eK7yNds/xDcFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAIYoADCWdV3X7/WLAOCewTcFAIYoADBEAYAhCgAMUQBgiAIAQxQAGKIAwBAFAMb/Auzn2K93hIiAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_array = np.array(df.iloc[random.randint(1, len(df))]).reshape(20, 20, 3)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_array)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -2.033875\n",
       "1      -0.348835\n",
       "2       0.113248\n",
       "3       1.223321\n",
       "4       0.160109\n",
       "          ...   \n",
       "5245    1.157565\n",
       "5246    1.424709\n",
       "5247   -0.375687\n",
       "5248   -0.478238\n",
       "5249   -0.750874\n",
       "Name: f_0, Length: 5250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_1190</th>\n",
       "      <th>f_1191</th>\n",
       "      <th>f_1192</th>\n",
       "      <th>f_1193</th>\n",
       "      <th>f_1194</th>\n",
       "      <th>f_1195</th>\n",
       "      <th>f_1196</th>\n",
       "      <th>f_1197</th>\n",
       "      <th>f_1198</th>\n",
       "      <th>f_1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.033875</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>-0.142131</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-1.470684</td>\n",
       "      <td>1.669562</td>\n",
       "      <td>-0.196530</td>\n",
       "      <td>-0.125239</td>\n",
       "      <td>-0.452284</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.111266</td>\n",
       "      <td>0.716084</td>\n",
       "      <td>0.060039</td>\n",
       "      <td>0.301279</td>\n",
       "      <td>-1.174846</td>\n",
       "      <td>-1.076498</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>-0.604012</td>\n",
       "      <td>-2.179176</td>\n",
       "      <td>0.558003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.348835</td>\n",
       "      <td>0.294815</td>\n",
       "      <td>-0.557577</td>\n",
       "      <td>-2.020773</td>\n",
       "      <td>-1.234715</td>\n",
       "      <td>1.633930</td>\n",
       "      <td>-1.680658</td>\n",
       "      <td>-0.358146</td>\n",
       "      <td>0.166122</td>\n",
       "      <td>-1.656990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735240</td>\n",
       "      <td>0.829781</td>\n",
       "      <td>1.521941</td>\n",
       "      <td>1.347946</td>\n",
       "      <td>0.754505</td>\n",
       "      <td>1.330642</td>\n",
       "      <td>-0.754453</td>\n",
       "      <td>0.582956</td>\n",
       "      <td>0.252671</td>\n",
       "      <td>1.495870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.113248</td>\n",
       "      <td>-0.607726</td>\n",
       "      <td>-0.947791</td>\n",
       "      <td>0.830851</td>\n",
       "      <td>0.998291</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>-1.493958</td>\n",
       "      <td>0.789572</td>\n",
       "      <td>-1.311018</td>\n",
       "      <td>0.848524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104698</td>\n",
       "      <td>0.616189</td>\n",
       "      <td>-1.035953</td>\n",
       "      <td>2.111387</td>\n",
       "      <td>-0.984415</td>\n",
       "      <td>1.148076</td>\n",
       "      <td>-1.433554</td>\n",
       "      <td>0.243372</td>\n",
       "      <td>0.170083</td>\n",
       "      <td>1.274795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.223321</td>\n",
       "      <td>-0.479048</td>\n",
       "      <td>-1.925789</td>\n",
       "      <td>1.680377</td>\n",
       "      <td>0.021840</td>\n",
       "      <td>-1.453307</td>\n",
       "      <td>0.605559</td>\n",
       "      <td>-0.019024</td>\n",
       "      <td>1.065448</td>\n",
       "      <td>0.717341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>-1.957863</td>\n",
       "      <td>-0.123384</td>\n",
       "      <td>1.505329</td>\n",
       "      <td>0.660290</td>\n",
       "      <td>-1.769443</td>\n",
       "      <td>-0.547756</td>\n",
       "      <td>-0.568122</td>\n",
       "      <td>0.244645</td>\n",
       "      <td>0.982116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160109</td>\n",
       "      <td>0.422684</td>\n",
       "      <td>-0.308029</td>\n",
       "      <td>0.227744</td>\n",
       "      <td>0.432854</td>\n",
       "      <td>0.608348</td>\n",
       "      <td>0.193832</td>\n",
       "      <td>1.035091</td>\n",
       "      <td>-0.538868</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416629</td>\n",
       "      <td>1.441766</td>\n",
       "      <td>0.212572</td>\n",
       "      <td>-0.994721</td>\n",
       "      <td>1.143999</td>\n",
       "      <td>-2.166923</td>\n",
       "      <td>-1.199248</td>\n",
       "      <td>-1.028636</td>\n",
       "      <td>0.752791</td>\n",
       "      <td>0.317169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5245</th>\n",
       "      <td>1.157565</td>\n",
       "      <td>-0.142219</td>\n",
       "      <td>1.043992</td>\n",
       "      <td>1.144946</td>\n",
       "      <td>1.195423</td>\n",
       "      <td>0.248978</td>\n",
       "      <td>-1.505100</td>\n",
       "      <td>-0.874137</td>\n",
       "      <td>-1.782724</td>\n",
       "      <td>0.261597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.195423</td>\n",
       "      <td>-0.255793</td>\n",
       "      <td>-0.154838</td>\n",
       "      <td>0.413029</td>\n",
       "      <td>-0.482939</td>\n",
       "      <td>-1.277953</td>\n",
       "      <td>-0.445082</td>\n",
       "      <td>1.195423</td>\n",
       "      <td>-0.924614</td>\n",
       "      <td>-0.432462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5246</th>\n",
       "      <td>1.424709</td>\n",
       "      <td>0.235910</td>\n",
       "      <td>1.356778</td>\n",
       "      <td>1.368099</td>\n",
       "      <td>-0.318862</td>\n",
       "      <td>1.039765</td>\n",
       "      <td>-0.986854</td>\n",
       "      <td>-0.330184</td>\n",
       "      <td>-1.383120</td>\n",
       "      <td>1.243559</td>\n",
       "      <td>...</td>\n",
       "      <td>1.424709</td>\n",
       "      <td>-1.066107</td>\n",
       "      <td>0.881258</td>\n",
       "      <td>-0.488691</td>\n",
       "      <td>-1.281223</td>\n",
       "      <td>-1.213291</td>\n",
       "      <td>0.122692</td>\n",
       "      <td>1.175627</td>\n",
       "      <td>-1.145360</td>\n",
       "      <td>0.451026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>-0.375687</td>\n",
       "      <td>1.524455</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>-0.007917</td>\n",
       "      <td>0.073809</td>\n",
       "      <td>-0.906909</td>\n",
       "      <td>-1.254247</td>\n",
       "      <td>1.606182</td>\n",
       "      <td>0.298557</td>\n",
       "      <td>0.053378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028349</td>\n",
       "      <td>-0.968204</td>\n",
       "      <td>-1.233815</td>\n",
       "      <td>1.626613</td>\n",
       "      <td>-0.191802</td>\n",
       "      <td>1.115823</td>\n",
       "      <td>0.380284</td>\n",
       "      <td>-0.293960</td>\n",
       "      <td>0.135104</td>\n",
       "      <td>1.381434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248</th>\n",
       "      <td>-0.478238</td>\n",
       "      <td>1.666142</td>\n",
       "      <td>0.049609</td>\n",
       "      <td>-0.428752</td>\n",
       "      <td>-0.362771</td>\n",
       "      <td>1.798104</td>\n",
       "      <td>-0.214314</td>\n",
       "      <td>0.775400</td>\n",
       "      <td>-0.379267</td>\n",
       "      <td>0.725914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428752</td>\n",
       "      <td>-1.121552</td>\n",
       "      <td>-0.379267</td>\n",
       "      <td>-0.593705</td>\n",
       "      <td>0.049609</td>\n",
       "      <td>1.765114</td>\n",
       "      <td>0.313533</td>\n",
       "      <td>-0.329781</td>\n",
       "      <td>-1.220524</td>\n",
       "      <td>0.033114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>-0.750874</td>\n",
       "      <td>0.267008</td>\n",
       "      <td>-0.155041</td>\n",
       "      <td>-0.179867</td>\n",
       "      <td>-0.155041</td>\n",
       "      <td>-0.303999</td>\n",
       "      <td>-0.279173</td>\n",
       "      <td>1.731765</td>\n",
       "      <td>0.564925</td>\n",
       "      <td>1.508328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.303999</td>\n",
       "      <td>-0.850180</td>\n",
       "      <td>0.937321</td>\n",
       "      <td>-1.594972</td>\n",
       "      <td>1.036626</td>\n",
       "      <td>1.582807</td>\n",
       "      <td>1.036626</td>\n",
       "      <td>-0.254346</td>\n",
       "      <td>0.664230</td>\n",
       "      <td>1.831071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5250 rows Ã— 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f_0       f_1       f_2       f_3       f_4       f_5       f_6  \\\n",
       "0    -2.033875  0.978446 -0.142131 -0.177117 -1.470684  1.669562 -0.196530   \n",
       "1    -0.348835  0.294815 -0.557577 -2.020773 -1.234715  1.633930 -1.680658   \n",
       "2     0.113248 -0.607726 -0.947791  0.830851  0.998291  0.498321 -1.493958   \n",
       "3     1.223321 -0.479048 -1.925789  1.680377  0.021840 -1.453307  0.605559   \n",
       "4     0.160109  0.422684 -0.308029  0.227744  0.432854  0.608348  0.193832   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5245  1.157565 -0.142219  1.043992  1.144946  1.195423  0.248978 -1.505100   \n",
       "5246  1.424709  0.235910  1.356778  1.368099 -0.318862  1.039765 -0.986854   \n",
       "5247 -0.375687  1.524455  0.012514 -0.007917  0.073809 -0.906909 -1.254247   \n",
       "5248 -0.478238  1.666142  0.049609 -0.428752 -0.362771  1.798104 -0.214314   \n",
       "5249 -0.750874  0.267008 -0.155041 -0.179867 -0.155041 -0.303999 -0.279173   \n",
       "\n",
       "           f_7       f_8       f_9  ...    f_1190    f_1191    f_1192  \\\n",
       "0    -0.125239 -0.452284 -0.128052  ... -1.111266  0.716084  0.060039   \n",
       "1    -0.358146  0.166122 -1.656990  ...  0.735240  0.829781  1.521941   \n",
       "2     0.789572 -1.311018  0.848524  ...  0.104698  0.616189 -1.035953   \n",
       "3    -0.019024  1.065448  0.717341  ...  0.360237 -1.957863 -0.123384   \n",
       "4     1.035091 -0.538868  0.778445  ...  0.416629  1.441766  0.212572   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5245 -0.874137 -1.782724  0.261597  ...  1.195423 -0.255793 -0.154838   \n",
       "5246 -0.330184 -1.383120  1.243559  ...  1.424709 -1.066107  0.881258   \n",
       "5247  1.606182  0.298557  0.053378  ... -0.028349 -0.968204 -1.233815   \n",
       "5248  0.775400 -0.379267  0.725914  ... -0.428752 -1.121552 -0.379267   \n",
       "5249  1.731765  0.564925  1.508328  ... -0.303999 -0.850180  0.937321   \n",
       "\n",
       "        f_1193    f_1194    f_1195    f_1196    f_1197    f_1198    f_1199  \n",
       "0     0.301279 -1.174846 -1.076498 -0.069452 -0.604012 -2.179176  0.558003  \n",
       "1     1.347946  0.754505  1.330642 -0.754453  0.582956  0.252671  1.495870  \n",
       "2     2.111387 -0.984415  1.148076 -1.433554  0.243372  0.170083  1.274795  \n",
       "3     1.505329  0.660290 -1.769443 -0.547756 -0.568122  0.244645  0.982116  \n",
       "4    -0.994721  1.143999 -2.166923 -1.199248 -1.028636  0.752791  0.317169  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5245  0.413029 -0.482939 -1.277953 -0.445082  1.195423 -0.924614 -0.432462  \n",
       "5246 -0.488691 -1.281223 -1.213291  0.122692  1.175627 -1.145360  0.451026  \n",
       "5247  1.626613 -0.191802  1.115823  0.380284 -0.293960  0.135104  1.381434  \n",
       "5248 -0.593705  0.049609  1.765114  0.313533 -0.329781 -1.220524  0.033114  \n",
       "5249 -1.594972  1.036626  1.582807  1.036626 -0.254346  0.664230  1.831071  \n",
       "\n",
       "[5250 rows x 1200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 11:14:15.906762: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 50400000 exceeds 10% of free system memory.\n",
      "2023-06-07 11:14:16.003272: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 64512000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "images = df.iloc[:, 0:].values.reshape(-1, 20, 20, 3) / 255.0\n",
    "import tensorflow as tf\n",
    "\n",
    "resized_images = tf.image.resize(images, [32, 32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5250, 32, 32, 3), dtype=float32, numpy=\n",
       "array([[[[-7.97598250e-03,  3.83704295e-03, -5.57375839e-04],\n",
       "         [-4.79036756e-03, -3.64895910e-04,  2.55092117e-03],\n",
       "         [-6.99334603e-04, -5.43762231e-03,  6.02724310e-03],\n",
       "         ...,\n",
       "         [ 1.09078651e-02, -4.67982283e-03, -3.62347811e-03],\n",
       "         [ 4.17698734e-03, -4.06364445e-04, -2.72413064e-03],\n",
       "         [-1.58556551e-03,  3.27473925e-03, -1.84302591e-03]],\n",
       "\n",
       "        [[-2.69158510e-03, -8.10001977e-04,  7.03437370e-04],\n",
       "         [-1.02158729e-03, -2.37236032e-03,  2.60378257e-03],\n",
       "         [ 9.17335798e-04, -4.16773325e-03,  4.52818023e-03],\n",
       "         ...,\n",
       "         [ 6.43136771e-03, -2.25070235e-03, -2.81473622e-03],\n",
       "         [ 1.44547364e-03, -9.64260078e-04, -8.57762760e-04],\n",
       "         [-2.67703040e-03,  3.12593533e-04,  8.03963980e-04]],\n",
       "\n",
       "        [[ 3.73637979e-03, -6.34533027e-03,  2.41204235e-03],\n",
       "         [ 3.53324809e-03, -4.69353795e-03,  2.45859241e-03],\n",
       "         [ 2.83561391e-03, -2.49564042e-03,  2.08402355e-03],\n",
       "         ...,\n",
       "         [ 8.84524256e-04,  5.98417595e-04, -1.38459704e-03],\n",
       "         [-1.71068741e-03, -1.62232795e-03,  1.47169584e-03],\n",
       "         [-3.64095205e-03, -3.17727402e-03,  3.76183703e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.56474426e-04,  4.23025293e-03,  3.16210347e-03],\n",
       "         [-3.29902163e-04,  2.87080323e-03,  3.14262323e-03],\n",
       "         [-1.19832007e-03,  9.94291622e-04,  3.29478993e-03],\n",
       "         ...,\n",
       "         [ 4.64632595e-03, -2.59032939e-04,  2.18205852e-03],\n",
       "         [ 2.29844963e-03, -2.51896679e-03,  1.56239956e-03],\n",
       "         [ 1.69852516e-04, -4.15784772e-03,  1.24917179e-03]],\n",
       "\n",
       "        [[-5.45488670e-03,  1.97553169e-03,  1.77970994e-03],\n",
       "         [-3.67152644e-03,  1.05821528e-03,  2.86818366e-03],\n",
       "         [-1.47881662e-03, -3.03466106e-04,  4.18503582e-03],\n",
       "         ...,\n",
       "         [-1.44624617e-04, -2.18713610e-03,  9.31592309e-04],\n",
       "         [-7.77082751e-04, -4.83490806e-03,  1.30726746e-03],\n",
       "         [-1.21031504e-03, -6.71947608e-03,  1.71974371e-03]],\n",
       "\n",
       "        [[-9.73552093e-03, -1.27553940e-04,  5.82670968e-04],\n",
       "         [-6.08423864e-03, -7.54181063e-04,  2.47999467e-03],\n",
       "         [-1.57361757e-03, -1.75257074e-03,  4.62379679e-03],\n",
       "         ...,\n",
       "         [-4.14377591e-03, -3.94299952e-03, -1.81496958e-04],\n",
       "         [-3.34804668e-03, -6.65393891e-03,  1.11173070e-03],\n",
       "         [-2.36867461e-03, -8.54578707e-03,  2.18824763e-03]]],\n",
       "\n",
       "\n",
       "       [[[-1.36798073e-03,  1.15613558e-03, -2.18657521e-03],\n",
       "         [-4.23650164e-03, -1.46805786e-03,  1.57336285e-03],\n",
       "         [-7.84123782e-03, -4.62717563e-03,  6.04781229e-03],\n",
       "         ...,\n",
       "         [-4.52430686e-03,  5.29576745e-03, -3.92657146e-03],\n",
       "         [-5.87794092e-03,  5.13408519e-03, -4.89880797e-03],\n",
       "         [-7.21228961e-03,  4.58412478e-03, -5.26481308e-03]],\n",
       "\n",
       "        [[ 8.38877284e-04,  1.02477847e-04, -8.94030672e-04],\n",
       "         [-5.21426322e-04, -2.07666098e-03,  1.26360706e-03],\n",
       "         [-2.28850683e-03, -4.60653845e-03,  3.76384938e-03],\n",
       "         ...,\n",
       "         [-3.39332083e-03,  4.24121460e-03, -1.57154258e-03],\n",
       "         [-3.41819529e-03,  3.43169412e-03, -2.34586629e-03],\n",
       "         [-3.47750401e-03,  2.66972464e-03, -2.82080611e-03]],\n",
       "\n",
       "        [[ 3.16832820e-03, -1.03136071e-03,  5.91254677e-04],\n",
       "         [ 3.86940967e-03, -2.71074707e-03,  8.39950633e-04],\n",
       "         [ 4.59868927e-03, -4.52950271e-03,  9.91762732e-04],\n",
       "         ...,\n",
       "         [-1.60158670e-03,  2.86587817e-03,  1.30602566e-03],\n",
       "         [ 6.30879658e-05,  1.44832267e-03,  8.40660301e-04],\n",
       "         [ 1.59736874e-03,  5.62718720e-04,  2.86215043e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.61685888e-03,  6.27395371e-03, -7.37467455e-03],\n",
       "         [ 6.14321791e-04,  5.72198443e-03, -2.97491997e-03],\n",
       "         [-8.15685897e-04,  4.40590410e-03,  2.74536549e-03],\n",
       "         ...,\n",
       "         [-2.27953447e-03,  2.78791762e-03, -1.37774739e-03],\n",
       "         [ 4.15147166e-04,  5.29565942e-03,  4.95668035e-04],\n",
       "         [ 2.51994841e-03,  7.01661780e-03,  2.08061701e-03]],\n",
       "\n",
       "        [[ 1.02884090e-03,  3.71235912e-03, -5.68388309e-03],\n",
       "         [-3.35748540e-04,  2.64927838e-03, -2.24282732e-03],\n",
       "         [-2.10769475e-03,  1.12032890e-03,  2.32887222e-03],\n",
       "         ...,\n",
       "         [ 6.52611256e-04,  4.24174313e-03, -2.22027581e-03],\n",
       "         [ 1.67564698e-03,  3.96362925e-03,  1.19839853e-03],\n",
       "         [ 2.48532649e-03,  3.66989803e-03,  4.15331451e-03]],\n",
       "\n",
       "        [[ 2.69371929e-04,  1.44601287e-03, -3.92616494e-03],\n",
       "         [-1.25192967e-03,  2.48820288e-05, -1.51994685e-03],\n",
       "         [-3.11475247e-03, -1.59808388e-03,  1.78397307e-03],\n",
       "         ...,\n",
       "         [ 2.97729415e-03,  5.26509108e-03, -2.44334573e-03],\n",
       "         [ 2.58042780e-03,  2.84032687e-03,  2.00530747e-03],\n",
       "         [ 2.28610379e-03,  9.90866567e-04,  5.86615596e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 4.44109028e-04, -2.38323794e-03, -3.71682760e-03],\n",
       "         [ 1.67529099e-03,  3.72182811e-04, -1.23575330e-03],\n",
       "         [ 2.68843281e-03,  3.86370998e-03,  1.51073409e-03],\n",
       "         ...,\n",
       "         [ 5.18724276e-03, -2.16144836e-03, -1.99597562e-03],\n",
       "         [ 3.26777366e-03, -5.39743621e-03, -1.10172469e-03],\n",
       "         [ 1.23695598e-03, -7.46080885e-03, -4.09532862e-04]],\n",
       "\n",
       "        [[-4.94118431e-04, -6.50762580e-04, -1.40449521e-03],\n",
       "         [ 7.30754109e-05,  1.70690019e-03,  6.77359058e-05],\n",
       "         [ 7.61546311e-04,  4.39008977e-03,  1.61493663e-03],\n",
       "         ...,\n",
       "         [ 2.22332636e-03, -2.93121673e-03, -1.16440817e-03],\n",
       "         [ 1.53760228e-03, -5.37542673e-03, -1.58640463e-03],\n",
       "         [ 6.86899060e-04, -6.95909699e-03, -2.09667999e-03]],\n",
       "\n",
       "        [[-1.30837434e-03,  1.21512625e-03,  1.70619122e-03],\n",
       "         [-1.72413490e-03,  3.05788382e-03,  1.85679051e-03],\n",
       "         [-1.63091579e-03,  4.72842855e-03,  1.83601596e-03],\n",
       "         ...,\n",
       "         [-1.47597620e-03, -3.58836469e-03, -8.51872028e-05],\n",
       "         [-3.85972846e-04, -5.21650957e-03, -2.05335347e-03],\n",
       "         [ 4.20590164e-04, -6.33938285e-03, -3.97435576e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.04254344e-03, -1.97367230e-03, -6.67109154e-03],\n",
       "         [ 6.34044409e-05, -5.62599860e-04, -6.34353142e-03],\n",
       "         [-1.43594958e-03,  1.01086032e-03, -5.92980720e-03],\n",
       "         ...,\n",
       "         [ 3.73894931e-03,  1.56994862e-03,  8.67548399e-04],\n",
       "         [ 1.45464740e-03,  3.45658651e-03,  4.91168257e-03],\n",
       "         [-1.69855077e-04,  4.61028563e-03,  7.86059536e-03]],\n",
       "\n",
       "        [[ 1.08084036e-03, -2.85285455e-03, -8.76560807e-04],\n",
       "         [ 1.27527094e-03, -1.47984351e-03, -2.43799156e-03],\n",
       "         [ 1.07908738e-03,  3.98350647e-04, -4.14998550e-03],\n",
       "         ...,\n",
       "         [-2.32887454e-04,  3.10604367e-03, -2.44208984e-03],\n",
       "         [ 9.26479697e-05,  2.86191818e-03,  2.48580589e-03],\n",
       "         [ 5.81269793e-04,  2.27293442e-03,  6.60652947e-03]],\n",
       "\n",
       "        [[ 9.33769974e-04, -3.49625479e-03,  4.26114677e-03],\n",
       "         [ 2.08129524e-03, -2.34933267e-03,  1.03631313e-03],\n",
       "         [ 2.95943674e-03, -4.50804393e-04, -2.59317597e-03],\n",
       "         ...,\n",
       "         [-3.46814469e-03,  3.96695640e-03, -4.75292373e-03],\n",
       "         [-1.15209562e-03,  2.34492170e-03,  3.52519099e-04],\n",
       "         [ 9.54401738e-04,  6.66993845e-04,  4.99919616e-03]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-1.47328095e-03,  5.97825553e-03,  4.90759776e-05],\n",
       "         [-8.42304085e-04,  3.48940236e-03, -1.52836635e-03],\n",
       "         [-3.36521014e-04,  6.65029627e-04, -3.26104881e-03],\n",
       "         ...,\n",
       "         [ 2.84440350e-04,  5.22709219e-03, -9.02397092e-04],\n",
       "         [ 2.75827036e-03,  6.48904592e-03, -1.06765295e-03],\n",
       "         [ 4.61614691e-03,  7.26024015e-03, -1.31303282e-03]],\n",
       "\n",
       "        [[ 3.40527622e-05,  2.08723149e-03, -2.47483142e-03],\n",
       "         [-1.01782614e-04,  7.02587888e-04, -1.85919087e-03],\n",
       "         [-4.32920235e-04, -7.04277772e-04, -1.06577505e-03],\n",
       "         ...,\n",
       "         [ 2.57830392e-03,  3.09316372e-03,  1.24905887e-03],\n",
       "         [ 1.75703282e-03,  2.78643868e-03,  2.40491098e-03],\n",
       "         [ 9.00393818e-04,  2.42275093e-03,  3.03369691e-03]],\n",
       "\n",
       "        [[ 1.79177406e-03, -2.81535857e-03, -5.48449112e-03],\n",
       "         [ 9.19799088e-04, -2.77154078e-03, -2.18719873e-03],\n",
       "         [-1.72517059e-04, -2.36591278e-03,  1.69881759e-03],\n",
       "         ...,\n",
       "         [ 5.10283699e-03,  2.33423896e-04,  3.51506728e-03],\n",
       "         [ 4.09321394e-04, -1.88516860e-03,  6.32723328e-03],\n",
       "         [-3.60658369e-03, -3.54148285e-03,  8.08651932e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.11327337e-04, -3.25103290e-03, -4.89858398e-03],\n",
       "         [-2.17874860e-03, -1.24636700e-03,  9.73006245e-04],\n",
       "         [-4.41502267e-03,  1.29225058e-03,  7.67619675e-03],\n",
       "         ...,\n",
       "         [-2.54519284e-04, -9.02020838e-05,  6.07440481e-04],\n",
       "         [-2.22193985e-03,  1.83033349e-04, -1.98188075e-03],\n",
       "         [-3.81690916e-03,  2.79432628e-04, -4.08732798e-03]],\n",
       "\n",
       "        [[-2.92552938e-03, -4.70328145e-03, -2.71019596e-03],\n",
       "         [-3.15557304e-03, -1.02696486e-03,  1.77455996e-03],\n",
       "         [-3.43851093e-03,  3.63744376e-03,  6.72847964e-03],\n",
       "         ...,\n",
       "         [-8.17578286e-04,  2.07565073e-03,  1.19428639e-03],\n",
       "         [-1.59753580e-03,  1.34483201e-03,  1.11009390e-03],\n",
       "         [-2.30957568e-03,  4.59711737e-04,  1.14076585e-03]],\n",
       "\n",
       "        [[-4.67824284e-03, -5.71985543e-03, -4.31668304e-04],\n",
       "         [-3.45134339e-03, -8.47311690e-04,  2.68816203e-03],\n",
       "         [-2.10425770e-03,  5.35729388e-03,  5.93318604e-03],\n",
       "         ...,\n",
       "         [-9.42459097e-04,  3.79988272e-03,  1.79678155e-03],\n",
       "         [-9.77513380e-04,  2.21242523e-03,  3.69972782e-03],\n",
       "         [-1.15278468e-03,  5.29820274e-04,  5.41738700e-03]]],\n",
       "\n",
       "\n",
       "       [[[-1.87544338e-03,  6.53389143e-03,  1.94546737e-04],\n",
       "         [-1.79054146e-03,  3.05291195e-03,  3.19441524e-03],\n",
       "         [-1.62882346e-03, -1.14366959e-03,  6.51771994e-03],\n",
       "         ...,\n",
       "         [-3.72307142e-03, -4.51953243e-03,  5.26068732e-04],\n",
       "         [ 1.75916799e-03,  2.51924247e-03,  3.40092927e-04],\n",
       "         [ 6.14576833e-03,  8.15107115e-03,  4.53295506e-04]],\n",
       "\n",
       "        [[-1.47923431e-03,  7.21310731e-03,  1.09644796e-04],\n",
       "         [-1.50576618e-03,  2.96447240e-03,  3.49334092e-03],\n",
       "         [-1.60229160e-03, -2.12357938e-03,  7.13148993e-03],\n",
       "         ...,\n",
       "         [ 1.15879020e-03, -4.12332360e-03,  2.11798004e-03],\n",
       "         [ 4.29207645e-03,  5.78879379e-04,  2.62183230e-03],\n",
       "         [ 6.79668318e-03,  4.41538589e-03,  3.05695483e-03]],\n",
       "\n",
       "        [[-1.02238113e-03,  7.94083811e-03, -3.47208494e-04],\n",
       "         [-1.15150283e-03,  2.73528765e-03,  3.35840741e-03],\n",
       "         [-1.49439543e-03, -3.47468280e-03,  7.24671362e-03],\n",
       "         ...,\n",
       "         [ 7.12289894e-03, -3.42692551e-03,  3.92214628e-03],\n",
       "         [ 7.08676502e-03, -1.92117936e-03,  5.19213732e-03],\n",
       "         [ 7.05138920e-03, -5.57441846e-04,  5.98809356e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.23921093e-04, -3.22950538e-04, -2.17834138e-04],\n",
       "         [-2.80681415e-05, -1.55756669e-03,  3.09687899e-03],\n",
       "         [-1.03368133e-04, -2.99534039e-03,  6.92024594e-03],\n",
       "         ...,\n",
       "         [-5.44807408e-04, -8.59905267e-04, -2.10867100e-03],\n",
       "         [-3.16767069e-03, -7.04504084e-04, -1.62402249e-03],\n",
       "         [-4.99659963e-03, -7.15117436e-04, -1.18005613e-03]],\n",
       "\n",
       "        [[-4.11895744e-04, -3.52900987e-03, -5.85742528e-04],\n",
       "         [-6.20612991e-04, -2.55971262e-03,  9.35417134e-04],\n",
       "         [-6.94144168e-04, -1.43425656e-03,  2.59327935e-03],\n",
       "         ...,\n",
       "         [-5.06399665e-04,  3.08879255e-03, -3.76014388e-04],\n",
       "         [-2.03791936e-03, -2.98945757e-04, -3.76014505e-04],\n",
       "         [-3.24600330e-03, -3.22983135e-03, -4.36153379e-04]],\n",
       "\n",
       "        [[-1.03450997e-03, -6.27417257e-03, -8.40448367e-04],\n",
       "         [-1.11941190e-03, -3.47240851e-03, -7.55546382e-04],\n",
       "         [-1.18409912e-03, -1.85490499e-04, -8.36405379e-04],\n",
       "         ...,\n",
       "         [-9.25026834e-05,  6.39643054e-03,  1.00717973e-03],\n",
       "         [-6.42343890e-04,  3.36050056e-04,  6.10970543e-04],\n",
       "         [-1.29325874e-03, -4.78636706e-03,  1.29859545e-04]]],\n",
       "\n",
       "\n",
       "       [[[-2.94460519e-03,  1.04708970e-03, -6.08003291e-04],\n",
       "         [-1.96493603e-03,  3.22986511e-04, -8.63569090e-04],\n",
       "         [-7.29701307e-04, -1.45550817e-04, -9.79182310e-04],\n",
       "         ...,\n",
       "         [-4.55101905e-03,  4.22340818e-03,  3.71836149e-03],\n",
       "         [ 2.43883114e-04,  1.01666525e-03,  1.98416458e-03],\n",
       "         [ 4.16255882e-03, -1.58158748e-03,  3.65580840e-04]],\n",
       "\n",
       "        [[-1.87974749e-03, -2.99936952e-03, -3.37663316e-03],\n",
       "         [-1.53366872e-03, -1.18911138e-03, -2.71908357e-03],\n",
       "         [-1.18492800e-03,  1.30531774e-03, -1.70062343e-03],\n",
       "         ...,\n",
       "         [-8.12788494e-05,  4.11217567e-04,  4.38655959e-03],\n",
       "         [ 2.70370441e-03, -6.15229132e-04,  4.41736449e-03],\n",
       "         [ 4.75887908e-03, -1.36861601e-03,  4.02869098e-03]],\n",
       "\n",
       "        [[-6.14088203e-04, -7.38049811e-03, -6.86936639e-03],\n",
       "         [-9.36207711e-04, -2.96932529e-03, -4.91801463e-03],\n",
       "         [-1.51199149e-03,  2.53028423e-03, -2.29846500e-03],\n",
       "         ...,\n",
       "         [ 5.24985557e-03, -4.31522913e-03,  4.45843767e-03],\n",
       "         [ 5.51264640e-03, -2.52740923e-03,  6.82318164e-03],\n",
       "         [ 5.25175640e-03, -9.06163477e-04,  8.11165944e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.94230030e-04, -7.72295753e-04, -4.92828293e-03],\n",
       "         [-2.66469968e-03, -1.91701762e-03, -1.49944145e-03],\n",
       "         [-5.12071839e-03, -3.10927816e-03,  2.70636613e-03],\n",
       "         ...,\n",
       "         [-1.03889033e-03,  1.99386850e-04, -1.24653755e-03],\n",
       "         [-3.91324563e-03,  7.99890608e-04, -1.36671436e-03],\n",
       "         [-5.77408401e-03,  1.45477813e-03, -1.45988935e-03]],\n",
       "\n",
       "        [[-8.75738973e-04, -1.28342735e-03, -2.37262459e-03],\n",
       "         [-2.39316095e-03,  2.95224134e-04, -7.56703143e-04],\n",
       "         [-3.96610796e-03,  2.39451462e-03,  1.06800662e-03],\n",
       "         ...,\n",
       "         [ 1.56088360e-03,  3.62366508e-03,  1.27717515e-03],\n",
       "         [-1.22904358e-03,  2.80448515e-03,  2.62307911e-03],\n",
       "         [-3.42531223e-03,  2.17888132e-03,  3.38977645e-03]],\n",
       "\n",
       "        [[-1.38687063e-03, -1.58158748e-03, -4.13286471e-04],\n",
       "         [-1.89800223e-03,  2.25190027e-03, -1.57720642e-04],\n",
       "         [-2.46998295e-03,  7.07722642e-03, -1.27296138e-04],\n",
       "         ...,\n",
       "         [ 3.60274827e-03,  6.04887819e-03,  3.42020113e-03],\n",
       "         [ 1.21746678e-03,  4.18081367e-03,  5.81765221e-03],\n",
       "         [-9.97436931e-04,  2.60482449e-03,  7.18066981e-03]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 33s 0us/step\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 12:13:48.915762: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [5250]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-06-07 12:13:48.916093: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [5250]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/132 [============================>.] - ETA: 0s - loss: 0.7925 - accuracy: 0.6520"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 12:13:56.733288: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [5250]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-06-07 12:13:56.733514: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [5250]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 9s 56ms/step - loss: 0.7916 - accuracy: 0.6518 - val_loss: 0.5844 - val_accuracy: 0.7407\n",
      "Epoch 2/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.6773 - accuracy: 0.6714 - val_loss: 0.9742 - val_accuracy: 0.2827\n",
      "Epoch 3/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.6895 - accuracy: 0.6615 - val_loss: 0.5767 - val_accuracy: 0.7368\n",
      "Epoch 4/200\n",
      "132/132 [==============================] - 6s 47ms/step - loss: 0.6172 - accuracy: 0.7164 - val_loss: 0.5716 - val_accuracy: 0.7417\n",
      "Epoch 5/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.6219 - accuracy: 0.7157 - val_loss: 0.6006 - val_accuracy: 0.7232\n",
      "Epoch 6/200\n",
      "132/132 [==============================] - 6s 47ms/step - loss: 0.6277 - accuracy: 0.7062 - val_loss: 0.6130 - val_accuracy: 0.7300\n",
      "Epoch 7/200\n",
      "132/132 [==============================] - 6s 47ms/step - loss: 0.6142 - accuracy: 0.7263 - val_loss: 0.6396 - val_accuracy: 0.7310\n",
      "Epoch 8/200\n",
      "132/132 [==============================] - 6s 47ms/step - loss: 0.6014 - accuracy: 0.7332 - val_loss: 0.6025 - val_accuracy: 0.7261\n",
      "Epoch 9/200\n",
      "132/132 [==============================] - 6s 47ms/step - loss: 0.5968 - accuracy: 0.7332 - val_loss: 0.5962 - val_accuracy: 0.7466\n",
      "Epoch 10/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.6057 - accuracy: 0.7304 - val_loss: 0.5903 - val_accuracy: 0.7281\n",
      "Epoch 11/200\n",
      "132/132 [==============================] - 6s 47ms/step - loss: 0.5939 - accuracy: 0.7339 - val_loss: 0.6442 - val_accuracy: 0.7144\n",
      "Epoch 12/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5877 - accuracy: 0.7356 - val_loss: 0.5788 - val_accuracy: 0.7349\n",
      "Epoch 13/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5882 - accuracy: 0.7341 - val_loss: 0.5802 - val_accuracy: 0.7388\n",
      "Epoch 14/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5858 - accuracy: 0.7313 - val_loss: 0.5947 - val_accuracy: 0.7300\n",
      "Epoch 15/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5855 - accuracy: 0.7332 - val_loss: 0.6234 - val_accuracy: 0.7173\n",
      "Epoch 16/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5859 - accuracy: 0.7334 - val_loss: 0.6275 - val_accuracy: 0.7251\n",
      "Epoch 17/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5817 - accuracy: 0.7372 - val_loss: 0.5953 - val_accuracy: 0.7183\n",
      "Epoch 18/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5839 - accuracy: 0.7311 - val_loss: 0.5831 - val_accuracy: 0.7310\n",
      "Epoch 19/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5811 - accuracy: 0.7351 - val_loss: 0.5764 - val_accuracy: 0.7378\n",
      "Epoch 20/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5824 - accuracy: 0.7334 - val_loss: 0.6008 - val_accuracy: 0.7203\n",
      "Epoch 21/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5800 - accuracy: 0.7353 - val_loss: 0.5823 - val_accuracy: 0.7310\n",
      "Epoch 22/200\n",
      "132/132 [==============================] - 7s 51ms/step - loss: 0.5790 - accuracy: 0.7358 - val_loss: 0.5685 - val_accuracy: 0.7456\n",
      "Epoch 23/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5816 - accuracy: 0.7341 - val_loss: 0.5793 - val_accuracy: 0.7388\n",
      "Epoch 24/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5813 - accuracy: 0.7334 - val_loss: 0.5829 - val_accuracy: 0.7310\n",
      "Epoch 25/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5763 - accuracy: 0.7389 - val_loss: 0.5795 - val_accuracy: 0.7339\n",
      "Epoch 26/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5854 - accuracy: 0.7289 - val_loss: 0.5614 - val_accuracy: 0.7524\n",
      "Epoch 27/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5850 - accuracy: 0.7299 - val_loss: 0.5750 - val_accuracy: 0.7388\n",
      "Epoch 28/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5811 - accuracy: 0.7346 - val_loss: 0.5804 - val_accuracy: 0.7329\n",
      "Epoch 29/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5801 - accuracy: 0.7356 - val_loss: 0.5938 - val_accuracy: 0.7193\n",
      "Epoch 30/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5798 - accuracy: 0.7351 - val_loss: 0.5946 - val_accuracy: 0.7183\n",
      "Epoch 31/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5809 - accuracy: 0.7334 - val_loss: 0.5873 - val_accuracy: 0.7261\n",
      "Epoch 32/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5821 - accuracy: 0.7344 - val_loss: 0.5822 - val_accuracy: 0.7398\n",
      "Epoch 33/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5845 - accuracy: 0.7311 - val_loss: 0.6016 - val_accuracy: 0.7135\n",
      "Epoch 34/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5823 - accuracy: 0.7327 - val_loss: 0.5773 - val_accuracy: 0.7359\n",
      "Epoch 35/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5775 - accuracy: 0.7375 - val_loss: 0.5874 - val_accuracy: 0.7290\n",
      "Epoch 36/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5826 - accuracy: 0.7320 - val_loss: 0.5824 - val_accuracy: 0.7349\n",
      "Epoch 37/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5830 - accuracy: 0.7322 - val_loss: 0.5754 - val_accuracy: 0.7378\n",
      "Epoch 38/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5818 - accuracy: 0.7332 - val_loss: 0.5629 - val_accuracy: 0.7505\n",
      "Epoch 39/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5833 - accuracy: 0.7318 - val_loss: 0.5825 - val_accuracy: 0.7329\n",
      "Epoch 40/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5754 - accuracy: 0.7398 - val_loss: 0.5910 - val_accuracy: 0.7339\n",
      "Epoch 41/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5832 - accuracy: 0.7311 - val_loss: 0.5832 - val_accuracy: 0.7310\n",
      "Epoch 42/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5882 - accuracy: 0.7270 - val_loss: 0.5875 - val_accuracy: 0.7271\n",
      "Epoch 43/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5785 - accuracy: 0.7356 - val_loss: 0.5837 - val_accuracy: 0.7310\n",
      "Epoch 44/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5837 - accuracy: 0.7315 - val_loss: 0.5833 - val_accuracy: 0.7300\n",
      "Epoch 45/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5818 - accuracy: 0.7337 - val_loss: 0.5689 - val_accuracy: 0.7446\n",
      "Epoch 46/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5803 - accuracy: 0.7341 - val_loss: 0.5947 - val_accuracy: 0.7183\n",
      "Epoch 47/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5816 - accuracy: 0.7330 - val_loss: 0.5750 - val_accuracy: 0.7398\n",
      "Epoch 48/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5767 - accuracy: 0.7386 - val_loss: 0.5911 - val_accuracy: 0.7232\n",
      "Epoch 49/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5766 - accuracy: 0.7384 - val_loss: 0.5684 - val_accuracy: 0.7446\n",
      "Epoch 50/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5768 - accuracy: 0.7372 - val_loss: 0.5642 - val_accuracy: 0.7495\n",
      "Epoch 51/200\n",
      "132/132 [==============================] - 7s 50ms/step - loss: 0.5883 - accuracy: 0.7266 - val_loss: 0.5897 - val_accuracy: 0.7242\n",
      "Epoch 52/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5814 - accuracy: 0.7332 - val_loss: 0.5794 - val_accuracy: 0.7339\n",
      "Epoch 53/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5830 - accuracy: 0.7315 - val_loss: 0.5649 - val_accuracy: 0.7485\n",
      "Epoch 54/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5773 - accuracy: 0.7367 - val_loss: 0.5704 - val_accuracy: 0.7437\n",
      "Epoch 55/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5822 - accuracy: 0.7332 - val_loss: 0.5891 - val_accuracy: 0.7242\n",
      "Epoch 56/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5833 - accuracy: 0.7318 - val_loss: 0.5651 - val_accuracy: 0.7476\n",
      "Epoch 57/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5751 - accuracy: 0.7391 - val_loss: 0.5879 - val_accuracy: 0.7281\n",
      "Epoch 58/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5843 - accuracy: 0.7306 - val_loss: 0.5562 - val_accuracy: 0.7563\n",
      "Epoch 59/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5784 - accuracy: 0.7358 - val_loss: 0.5928 - val_accuracy: 0.7203\n",
      "Epoch 60/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5816 - accuracy: 0.7341 - val_loss: 0.5871 - val_accuracy: 0.7261\n",
      "Epoch 61/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5817 - accuracy: 0.7327 - val_loss: 0.5697 - val_accuracy: 0.7437\n",
      "Epoch 62/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5821 - accuracy: 0.7327 - val_loss: 0.5833 - val_accuracy: 0.7310\n",
      "Epoch 63/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5758 - accuracy: 0.7391 - val_loss: 0.5915 - val_accuracy: 0.7222\n",
      "Epoch 64/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5833 - accuracy: 0.7306 - val_loss: 0.5733 - val_accuracy: 0.7398\n",
      "Epoch 65/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5799 - accuracy: 0.7360 - val_loss: 0.5773 - val_accuracy: 0.7359\n",
      "Epoch 66/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5835 - accuracy: 0.7325 - val_loss: 0.5973 - val_accuracy: 0.7154\n",
      "Epoch 67/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5824 - accuracy: 0.7322 - val_loss: 0.5786 - val_accuracy: 0.7349\n",
      "Epoch 68/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5852 - accuracy: 0.7299 - val_loss: 0.5754 - val_accuracy: 0.7378\n",
      "Epoch 69/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5829 - accuracy: 0.7313 - val_loss: 0.5970 - val_accuracy: 0.7164\n",
      "Epoch 70/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5791 - accuracy: 0.7353 - val_loss: 0.5652 - val_accuracy: 0.7524\n",
      "Epoch 71/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5799 - accuracy: 0.7353 - val_loss: 0.5736 - val_accuracy: 0.7427\n",
      "Epoch 72/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5814 - accuracy: 0.7330 - val_loss: 0.5818 - val_accuracy: 0.7329\n",
      "Epoch 73/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5839 - accuracy: 0.7322 - val_loss: 0.5745 - val_accuracy: 0.7388\n",
      "Epoch 74/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5831 - accuracy: 0.7308 - val_loss: 0.5805 - val_accuracy: 0.7329\n",
      "Epoch 75/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5812 - accuracy: 0.7332 - val_loss: 0.5788 - val_accuracy: 0.7349\n",
      "Epoch 76/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5852 - accuracy: 0.7289 - val_loss: 0.5715 - val_accuracy: 0.7446\n",
      "Epoch 77/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5741 - accuracy: 0.7405 - val_loss: 0.5973 - val_accuracy: 0.7154\n",
      "Epoch 78/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5810 - accuracy: 0.7337 - val_loss: 0.5902 - val_accuracy: 0.7232\n",
      "Epoch 79/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5796 - accuracy: 0.7344 - val_loss: 0.5798 - val_accuracy: 0.7339\n",
      "Epoch 80/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5855 - accuracy: 0.7287 - val_loss: 0.5795 - val_accuracy: 0.7339\n",
      "Epoch 81/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5820 - accuracy: 0.7327 - val_loss: 0.5804 - val_accuracy: 0.7329\n",
      "Epoch 82/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5809 - accuracy: 0.7334 - val_loss: 0.5843 - val_accuracy: 0.7290\n",
      "Epoch 83/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5868 - accuracy: 0.7273 - val_loss: 0.5946 - val_accuracy: 0.7183\n",
      "Epoch 84/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5785 - accuracy: 0.7346 - val_loss: 0.6081 - val_accuracy: 0.7115\n",
      "Epoch 85/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5808 - accuracy: 0.7346 - val_loss: 0.5822 - val_accuracy: 0.7320\n",
      "Epoch 86/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5794 - accuracy: 0.7346 - val_loss: 0.5862 - val_accuracy: 0.7271\n",
      "Epoch 87/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5773 - accuracy: 0.7379 - val_loss: 0.5844 - val_accuracy: 0.7290\n",
      "Epoch 88/200\n",
      "132/132 [==============================] - 7s 49ms/step - loss: 0.5771 - accuracy: 0.7363 - val_loss: 0.5752 - val_accuracy: 0.7388\n",
      "Epoch 89/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5828 - accuracy: 0.7308 - val_loss: 0.5788 - val_accuracy: 0.7359\n",
      "Epoch 90/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5801 - accuracy: 0.7344 - val_loss: 0.6027 - val_accuracy: 0.7105\n",
      "Epoch 91/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5750 - accuracy: 0.7379 - val_loss: 0.5831 - val_accuracy: 0.7388\n",
      "Epoch 92/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5798 - accuracy: 0.7351 - val_loss: 0.5833 - val_accuracy: 0.7300\n",
      "Epoch 93/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5782 - accuracy: 0.7370 - val_loss: 0.5793 - val_accuracy: 0.7339\n",
      "Epoch 94/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5840 - accuracy: 0.7301 - val_loss: 0.5626 - val_accuracy: 0.7534\n",
      "Epoch 95/200\n",
      "132/132 [==============================] - 7s 50ms/step - loss: 0.5839 - accuracy: 0.7296 - val_loss: 0.5654 - val_accuracy: 0.7505\n",
      "Epoch 96/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5782 - accuracy: 0.7365 - val_loss: 0.5919 - val_accuracy: 0.7251\n",
      "Epoch 97/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5822 - accuracy: 0.7322 - val_loss: 0.5792 - val_accuracy: 0.7359\n",
      "Epoch 98/200\n",
      "132/132 [==============================] - 7s 49ms/step - loss: 0.5804 - accuracy: 0.7346 - val_loss: 0.5796 - val_accuracy: 0.7339\n",
      "Epoch 99/200\n",
      "132/132 [==============================] - 7s 52ms/step - loss: 0.5757 - accuracy: 0.7393 - val_loss: 0.5784 - val_accuracy: 0.7359\n",
      "Epoch 100/200\n",
      "132/132 [==============================] - 7s 52ms/step - loss: 0.5841 - accuracy: 0.7311 - val_loss: 0.5844 - val_accuracy: 0.7300\n",
      "Epoch 101/200\n",
      "132/132 [==============================] - 7s 50ms/step - loss: 0.5837 - accuracy: 0.7308 - val_loss: 0.6046 - val_accuracy: 0.7076\n",
      "Epoch 102/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5796 - accuracy: 0.7346 - val_loss: 0.5803 - val_accuracy: 0.7329\n",
      "Epoch 103/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5852 - accuracy: 0.7287 - val_loss: 0.5676 - val_accuracy: 0.7456\n",
      "Epoch 104/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5795 - accuracy: 0.7344 - val_loss: 0.6052 - val_accuracy: 0.7066\n",
      "Epoch 105/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5806 - accuracy: 0.7339 - val_loss: 0.5871 - val_accuracy: 0.7320\n",
      "Epoch 106/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5768 - accuracy: 0.7375 - val_loss: 0.5975 - val_accuracy: 0.7193\n",
      "Epoch 107/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5766 - accuracy: 0.7375 - val_loss: 0.5896 - val_accuracy: 0.7281\n",
      "Epoch 108/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5808 - accuracy: 0.7337 - val_loss: 0.5899 - val_accuracy: 0.7232\n",
      "Epoch 109/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5789 - accuracy: 0.7358 - val_loss: 0.5865 - val_accuracy: 0.7281\n",
      "Epoch 110/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5786 - accuracy: 0.7353 - val_loss: 0.5639 - val_accuracy: 0.7485\n",
      "Epoch 111/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5808 - accuracy: 0.7332 - val_loss: 0.5844 - val_accuracy: 0.7300\n",
      "Epoch 112/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5807 - accuracy: 0.7334 - val_loss: 0.5705 - val_accuracy: 0.7456\n",
      "Epoch 113/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5776 - accuracy: 0.7367 - val_loss: 0.5733 - val_accuracy: 0.7398\n",
      "Epoch 114/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5843 - accuracy: 0.7296 - val_loss: 0.5972 - val_accuracy: 0.7173\n",
      "Epoch 115/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5732 - accuracy: 0.7412 - val_loss: 0.5967 - val_accuracy: 0.7193\n",
      "Epoch 116/200\n",
      "132/132 [==============================] - 7s 49ms/step - loss: 0.5821 - accuracy: 0.7320 - val_loss: 0.5890 - val_accuracy: 0.7242\n",
      "Epoch 117/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5822 - accuracy: 0.7320 - val_loss: 0.5756 - val_accuracy: 0.7378\n",
      "Epoch 118/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5801 - accuracy: 0.7334 - val_loss: 0.6008 - val_accuracy: 0.7115\n",
      "Epoch 119/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5796 - accuracy: 0.7341 - val_loss: 0.5808 - val_accuracy: 0.7339\n",
      "Epoch 120/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5838 - accuracy: 0.7304 - val_loss: 0.5677 - val_accuracy: 0.7456\n",
      "Epoch 121/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5829 - accuracy: 0.7325 - val_loss: 0.5932 - val_accuracy: 0.7203\n",
      "Epoch 122/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5787 - accuracy: 0.7353 - val_loss: 0.5832 - val_accuracy: 0.7300\n",
      "Epoch 123/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5789 - accuracy: 0.7356 - val_loss: 0.5866 - val_accuracy: 0.7271\n",
      "Epoch 124/200\n",
      "132/132 [==============================] - 7s 49ms/step - loss: 0.5794 - accuracy: 0.7351 - val_loss: 0.5754 - val_accuracy: 0.7378\n",
      "Epoch 125/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5821 - accuracy: 0.7320 - val_loss: 0.5749 - val_accuracy: 0.7398\n",
      "Epoch 126/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5810 - accuracy: 0.7330 - val_loss: 0.6056 - val_accuracy: 0.7086\n",
      "Epoch 127/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5798 - accuracy: 0.7337 - val_loss: 0.5797 - val_accuracy: 0.7359\n",
      "Epoch 128/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5846 - accuracy: 0.7301 - val_loss: 0.5873 - val_accuracy: 0.7290\n",
      "Epoch 129/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5811 - accuracy: 0.7332 - val_loss: 0.5843 - val_accuracy: 0.7290\n",
      "Epoch 130/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5788 - accuracy: 0.7346 - val_loss: 0.5548 - val_accuracy: 0.7602\n",
      "Epoch 131/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5842 - accuracy: 0.7294 - val_loss: 0.5764 - val_accuracy: 0.7368\n",
      "Epoch 132/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5853 - accuracy: 0.7292 - val_loss: 0.6063 - val_accuracy: 0.7066\n",
      "Epoch 133/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5864 - accuracy: 0.7285 - val_loss: 0.5761 - val_accuracy: 0.7398\n",
      "Epoch 134/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5775 - accuracy: 0.7372 - val_loss: 0.5637 - val_accuracy: 0.7515\n",
      "Epoch 135/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5884 - accuracy: 0.7256 - val_loss: 0.5767 - val_accuracy: 0.7378\n",
      "Epoch 136/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5788 - accuracy: 0.7360 - val_loss: 0.5682 - val_accuracy: 0.7456\n",
      "Epoch 137/200\n",
      "132/132 [==============================] - 6s 47ms/step - loss: 0.5820 - accuracy: 0.7320 - val_loss: 0.5682 - val_accuracy: 0.7446\n",
      "Epoch 138/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5806 - accuracy: 0.7330 - val_loss: 0.5899 - val_accuracy: 0.7251\n",
      "Epoch 139/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5805 - accuracy: 0.7334 - val_loss: 0.5817 - val_accuracy: 0.7320\n",
      "Epoch 140/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5838 - accuracy: 0.7301 - val_loss: 0.6253 - val_accuracy: 0.6901\n",
      "Epoch 141/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5797 - accuracy: 0.7341 - val_loss: 0.5909 - val_accuracy: 0.7232\n",
      "Epoch 142/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5866 - accuracy: 0.7273 - val_loss: 0.5656 - val_accuracy: 0.7476\n",
      "Epoch 143/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5784 - accuracy: 0.7348 - val_loss: 0.5716 - val_accuracy: 0.7427\n",
      "Epoch 144/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5813 - accuracy: 0.7325 - val_loss: 0.5808 - val_accuracy: 0.7329\n",
      "Epoch 145/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5797 - accuracy: 0.7341 - val_loss: 0.5776 - val_accuracy: 0.7368\n",
      "Epoch 146/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5823 - accuracy: 0.7320 - val_loss: 0.5686 - val_accuracy: 0.7446\n",
      "Epoch 147/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5784 - accuracy: 0.7351 - val_loss: 0.5833 - val_accuracy: 0.7300\n",
      "Epoch 148/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5853 - accuracy: 0.7285 - val_loss: 0.5671 - val_accuracy: 0.7495\n",
      "Epoch 149/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5836 - accuracy: 0.7304 - val_loss: 0.5754 - val_accuracy: 0.7378\n",
      "Epoch 150/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5801 - accuracy: 0.7337 - val_loss: 0.5936 - val_accuracy: 0.7212\n",
      "Epoch 151/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5830 - accuracy: 0.7320 - val_loss: 0.5671 - val_accuracy: 0.7456\n",
      "Epoch 152/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5827 - accuracy: 0.7308 - val_loss: 0.5947 - val_accuracy: 0.7183\n",
      "Epoch 153/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5828 - accuracy: 0.7313 - val_loss: 0.5975 - val_accuracy: 0.7173\n",
      "Epoch 154/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5819 - accuracy: 0.7320 - val_loss: 0.5973 - val_accuracy: 0.7154\n",
      "Epoch 155/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5870 - accuracy: 0.7266 - val_loss: 0.5866 - val_accuracy: 0.7300\n",
      "Epoch 156/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5823 - accuracy: 0.7325 - val_loss: 0.5833 - val_accuracy: 0.7300\n",
      "Epoch 157/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5809 - accuracy: 0.7332 - val_loss: 0.5762 - val_accuracy: 0.7378\n",
      "Epoch 158/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5820 - accuracy: 0.7318 - val_loss: 0.6066 - val_accuracy: 0.7086\n",
      "Epoch 159/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5753 - accuracy: 0.7384 - val_loss: 0.5876 - val_accuracy: 0.7271\n",
      "Epoch 160/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5838 - accuracy: 0.7320 - val_loss: 0.5815 - val_accuracy: 0.7320\n",
      "Epoch 161/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5830 - accuracy: 0.7308 - val_loss: 0.5802 - val_accuracy: 0.7339\n",
      "Epoch 162/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5820 - accuracy: 0.7320 - val_loss: 0.5533 - val_accuracy: 0.7622\n",
      "Epoch 163/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5836 - accuracy: 0.7304 - val_loss: 0.5631 - val_accuracy: 0.7515\n",
      "Epoch 164/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5822 - accuracy: 0.7320 - val_loss: 0.5736 - val_accuracy: 0.7398\n",
      "Epoch 165/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5820 - accuracy: 0.7320 - val_loss: 0.5635 - val_accuracy: 0.7515\n",
      "Epoch 166/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5824 - accuracy: 0.7315 - val_loss: 0.6018 - val_accuracy: 0.7105\n",
      "Epoch 167/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5812 - accuracy: 0.7327 - val_loss: 0.5701 - val_accuracy: 0.7446\n",
      "Epoch 168/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5765 - accuracy: 0.7375 - val_loss: 0.5795 - val_accuracy: 0.7349\n",
      "Epoch 169/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5763 - accuracy: 0.7379 - val_loss: 0.5923 - val_accuracy: 0.7222\n",
      "Epoch 170/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5766 - accuracy: 0.7372 - val_loss: 0.5828 - val_accuracy: 0.7310\n",
      "Epoch 171/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5814 - accuracy: 0.7327 - val_loss: 0.5678 - val_accuracy: 0.7466\n",
      "Epoch 172/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5819 - accuracy: 0.7322 - val_loss: 0.5495 - val_accuracy: 0.7632\n",
      "Epoch 173/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5823 - accuracy: 0.7320 - val_loss: 0.5805 - val_accuracy: 0.7329\n",
      "Epoch 174/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5802 - accuracy: 0.7334 - val_loss: 0.5851 - val_accuracy: 0.7290\n",
      "Epoch 175/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5847 - accuracy: 0.7289 - val_loss: 0.5807 - val_accuracy: 0.7329\n",
      "Epoch 176/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5886 - accuracy: 0.7249 - val_loss: 0.5694 - val_accuracy: 0.7446\n",
      "Epoch 177/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5774 - accuracy: 0.7363 - val_loss: 0.5793 - val_accuracy: 0.7339\n",
      "Epoch 178/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5769 - accuracy: 0.7370 - val_loss: 0.5825 - val_accuracy: 0.7310\n",
      "Epoch 179/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5739 - accuracy: 0.7396 - val_loss: 0.5854 - val_accuracy: 0.7281\n",
      "Epoch 180/200\n",
      "132/132 [==============================] - 7s 49ms/step - loss: 0.5759 - accuracy: 0.7375 - val_loss: 0.5676 - val_accuracy: 0.7456\n",
      "Epoch 181/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5786 - accuracy: 0.7348 - val_loss: 0.5559 - val_accuracy: 0.7563\n",
      "Epoch 182/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5823 - accuracy: 0.7313 - val_loss: 0.6046 - val_accuracy: 0.7086\n",
      "Epoch 183/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5791 - accuracy: 0.7344 - val_loss: 0.5783 - val_accuracy: 0.7349\n",
      "Epoch 184/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5817 - accuracy: 0.7318 - val_loss: 0.5891 - val_accuracy: 0.7242\n",
      "Epoch 185/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5833 - accuracy: 0.7301 - val_loss: 0.5881 - val_accuracy: 0.7251\n",
      "Epoch 186/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5751 - accuracy: 0.7384 - val_loss: 0.5846 - val_accuracy: 0.7290\n",
      "Epoch 187/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5755 - accuracy: 0.7379 - val_loss: 0.5793 - val_accuracy: 0.7339\n",
      "Epoch 188/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5767 - accuracy: 0.7367 - val_loss: 0.5856 - val_accuracy: 0.7281\n",
      "Epoch 189/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5788 - accuracy: 0.7346 - val_loss: 0.5774 - val_accuracy: 0.7359\n",
      "Epoch 190/200\n",
      "132/132 [==============================] - 7s 50ms/step - loss: 0.5766 - accuracy: 0.7367 - val_loss: 0.5892 - val_accuracy: 0.7242\n",
      "Epoch 191/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5783 - accuracy: 0.7351 - val_loss: 0.5931 - val_accuracy: 0.7203\n",
      "Epoch 192/200\n",
      "132/132 [==============================] - 7s 50ms/step - loss: 0.5817 - accuracy: 0.7318 - val_loss: 0.5715 - val_accuracy: 0.7417\n",
      "Epoch 193/200\n",
      "132/132 [==============================] - 7s 50ms/step - loss: 0.5766 - accuracy: 0.7367 - val_loss: 0.5625 - val_accuracy: 0.7505\n",
      "Epoch 194/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5808 - accuracy: 0.7327 - val_loss: 0.5880 - val_accuracy: 0.7251\n",
      "Epoch 195/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5811 - accuracy: 0.7325 - val_loss: 0.6019 - val_accuracy: 0.7115\n",
      "Epoch 196/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5840 - accuracy: 0.7294 - val_loss: 0.5909 - val_accuracy: 0.7222\n",
      "Epoch 197/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5824 - accuracy: 0.7311 - val_loss: 0.5803 - val_accuracy: 0.7329\n",
      "Epoch 198/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5805 - accuracy: 0.7330 - val_loss: 0.5871 - val_accuracy: 0.7261\n",
      "Epoch 199/200\n",
      "132/132 [==============================] - 6s 49ms/step - loss: 0.5791 - accuracy: 0.7344 - val_loss: 0.5606 - val_accuracy: 0.7524\n",
      "Epoch 200/200\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 0.5812 - accuracy: 0.7325 - val_loss: 0.5883 - val_accuracy: 0.7251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ec934bdf0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the data\n",
    "\n",
    "images = df.iloc[:, 0:].values.reshape(-1, 20, 20, 3) / 255.0\n",
    "resized_images = tf.image.resize(images, [32, 32])\n",
    "\n",
    "# Combine the images and labels into a single dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((resized_images, labels))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(len(df)).batch(32)\n",
    "\n",
    "# Determine the number of batches in the 80% train set\n",
    "num_batches = len(dataset)\n",
    "train_batches = int(0.8 * num_batches)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_dataset = dataset.take(train_batches)\n",
    "test_dataset = dataset.skip(train_batches)\n",
    "\n",
    "# Define the base model\n",
    "base_model = ResNet50(include_top=False, pooling='avg', weights='imagenet', input_shape=(32, 32, 3))\n",
    "\n",
    "# Make sure the base model is not trainable\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the final layers of the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_dataset, epochs=200, validation_data=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
