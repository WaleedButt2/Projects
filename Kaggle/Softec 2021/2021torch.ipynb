{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 22:47:56.804665: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAZY_MODULE_LOADING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim import Adam\n",
    "from torchviz import make_dot\n",
    "import shutil\n",
    "from transformers import  BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from bs4 import BeautifulSoup\n",
    "import xml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=df=pd.read_csv('train_set.csv')\n",
    "test=pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y = pd.read_csv('X_train.csv'),pd.read_csv('X_test.csv'),pd.read_csv('Y_train.csv')\n",
    "X_train=X_train.drop('Unnamed: 0',axis=1)\n",
    "X_test=X_test.drop('Unnamed: 0',axis=1)\n",
    "y=y.drop('Unnamed: 0',axis=1)\n",
    "def replace_repeating_newlines(df):\n",
    "    # Replace all repeating newlines with a single newline in the Text column\n",
    "    df['Text'] = df['Text'].str.replace('\\n+', '\\n')\n",
    "\n",
    "    # Replace all repeating newlines with a single newline in the Titles column\n",
    "    df['Titles'] = df['Titles'].str.replace('\\n+', '\\n')\n",
    "\n",
    "    # Replace all repeating newlines with a single newline in the Links column\n",
    "    df['Links'] = df['Links'].str.replace('\\n+', '\\n')\n",
    "\n",
    "    return df\n",
    "url_pattern = re.compile(r'https?://[^\\s]+.com')\n",
    "def ret_info(df):\n",
    "    links=[]\n",
    "    titles=[]\n",
    "    text=[]\n",
    "    for i in range(len(df)):\n",
    "        h=df.Html.loc[i].strip().lower()\n",
    "        soup=BeautifulSoup(h,'html.parser')\n",
    "        script_tags = soup.find_all(\"script\")\n",
    "        p_tags = soup.find_all(\"p\")\n",
    "        hreftags= soup.find_all('link')\n",
    "        script_contents = [script.string for script in script_tags if script.string]\n",
    "        http_links_in_scripts = [url_pattern.findall(script) for script in script_contents]\n",
    "        http_links_in_scripts = [url for sublist in http_links_in_scripts for url in sublist]\n",
    "        https_links = [link[\"href\"] for link in hreftags if link.has_attr(\"href\") and link[\"href\"].startswith(\"https://\")]\n",
    "        if(http_links_in_scripts==None and https_links==None):\n",
    "            links.append('')\n",
    "        links.append(set(http_links_in_scripts+https_links))\n",
    "        if soup.title!=None:\n",
    "            titles.append(soup.text.strip())\n",
    "        else:\n",
    "            titles.append('')\n",
    "        text.append([p.text.strip() for p in p_tags])\n",
    "    for i in range(len(text)):\n",
    "        text[i]='\\n'.join(text[i])\n",
    "        links[i]='\\n'.join(links[i])\n",
    "    return (links,text,titles)\n",
    "def find(ass,x):\n",
    "    for line in ass:\n",
    "        if line.find(x)!=-1:\n",
    "            return True\n",
    "    return False\n",
    "def Checker(df,links,text,titles):\n",
    "    a=0\n",
    "    for i in range(len(links)):\n",
    "            p=df.Source.loc[i].strip().lower()\n",
    "            h=df.Html.loc[i].strip().lower()    \n",
    "            if any(p in link for link in links[i] ):\n",
    "                a+=1\n",
    "                continue\n",
    "            for word in p.split():\n",
    "                if (titles[i]!=None and titles[i].find(word)!=-1):\n",
    "                    a+=1\n",
    "                else:\n",
    "                    if(p.split()==1):\n",
    "                        continue\n",
    "                    for word in p.split():\n",
    "                        if any(word in link for link in links[i] ):\n",
    "                            a+=1     \n",
    "                            break  \n",
    "                        elif (titles[i]!=None and titles[i].find(word)!=-1):\n",
    "                            a+=1\n",
    "                            break  \n",
    "                        elif (text[i]!=None and find(text[i],word)):\n",
    "                            a+=1\n",
    "                            break    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(links,text,titles) = ret_info(df)\n",
    "(linkst,textt,titlest) = ret_info(test)\n",
    "df=pd.DataFrame({'Titles':titles,'Links':links,'Text':text})\n",
    "fd=pd.DataFrame({'Titles':titlest,'Links':linkst,'Text':textt})\n",
    "a=0\n",
    "for i in range(len(df)):\n",
    "    if (df.Text.loc[i]=='' and df.Links.loc[i]=='' and df.Titles.loc[i]==''):\n",
    "        df=df.drop(i)\n",
    "        ss=ss.drop(i)\n",
    "        a+=1\n",
    "y=ss.Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2023/2872246211.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Text'] = df['Text'].str.replace('\\n+', '\\n')\n",
      "/tmp/ipykernel_2023/2872246211.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Titles'] = df['Titles'].str.replace('\\n+', '\\n')\n",
      "/tmp/ipykernel_2023/2872246211.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Links'] = df['Links'].str.replace('\\n+', '\\n')\n"
     ]
    }
   ],
   "source": [
    "df=replace_repeating_newlines(df)\n",
    "fd=replace_repeating_newlines(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/butt_sahab/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "max_length=128\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(row):\n",
    "    return f\"Title: {row['Titles']}\\nLinks: {row['Links']}\\nText: {row['Text']}\"\n",
    "\n",
    "X_train = X_train.apply(combine_columns, axis=1)\n",
    "X_val = X_val.apply(combine_columns, axis=1)\n",
    "fd = fd.apply(combine_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_categories = label_encoder.fit_transform(y.unique())\n",
    "labeled_train = label_encoder.transform(y_train)\n",
    "labeled_val = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "input_ids_val = []\n",
    "attention_masks_val = []\n",
    "input_ids_test = []\n",
    "attention_masks_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_train =tokenizer(X_train.to_list(), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "tokenized_data_val =tokenizer(X_val.to_list(), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "tokenized_data_test =tokenizer(fd.to_list(), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train.append(tokenized_data_train['input_ids'])\n",
    "attention_masks_train.append(tokenized_data_train['attention_mask'])\n",
    "input_ids_val.append(tokenized_data_val['input_ids'])\n",
    "attention_masks_val.append(tokenized_data_val['attention_mask'])\n",
    "input_ids_test.append(tokenized_data_test['input_ids'])\n",
    "attention_masks_test.append(tokenized_data_test['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test= torch.cat(attention_masks_test, dim=0)\n",
    "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "attention_masks_val= torch.cat(attention_masks_val, dim=0)\n",
    "labeled_train = torch.tensor(labeled_train)\n",
    "labeled_val = torch.tensor(labeled_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Train = TensorDataset(input_ids_train,attention_masks_train,labeled_train)\n",
    "Dataset_Test = TensorDataset(input_ids_test,attention_masks_test)\n",
    "Dataset_Val = TensorDataset(input_ids_val,attention_masks_val,labeled_val)\n",
    "Train = DataLoader(Dataset_Train, batch_size=batch_size, shuffle=True)\n",
    "Test = DataLoader(Dataset_Test, batch_size=batch_size, shuffle=False)\n",
    "Val = DataLoader(Dataset_Val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=596, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = len(encoded_categories), # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "total_t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/butt_sahab/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer=optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f94eb8159e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for param in model.parameters():\n",
    "    i+=1\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for param in model.parameters():\n",
    "    i+=1\n",
    "    if(i<20):\n",
    "        param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "        print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "def train(train_dataloader,validation_dataloader, model, optimizer):\n",
    "    t0 = time.time()\n",
    "    total_trian_accraucy=0\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "            # Calculate elapsed time in minutes.\n",
    "            # Report progress.\n",
    "           # print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss=output.loss\n",
    "        logits=output.logits\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_trian_accraucy += flat_accuracy(logits, label_ids)\n",
    "        avg_train_accruacy=total_trian_accraucy/len(train_dataloader)\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "    print(\"  Average training Accuracy: {0:.3f}\".format(avg_train_accruacy))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask, \n",
    "                                labels=b_labels)\n",
    "            loss=output.loss\n",
    "            logits=output.logits\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "\n",
    "    print(\"Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # Measure how long the validation run took.\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(test_dataloader, model):\n",
    "    pred=[]\n",
    "    for (X,mask) in Test:\n",
    "        X,mask =X.to(device),mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(X,token_type_ids=None,attention_mask=mask)\n",
    "            logits=output.logits.detach().cpu().numpy()\n",
    "            pred.append(np.argmax(logits, axis=1).flatten())\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "======== Epoch 1 / 100 ========\n",
      "\n",
      "  Average training loss: 6.308\n",
      "  Average training Accuracy: 0.017\n",
      "Validation Accuracy: 0.07\n",
      "  Validation Loss: 6.16\n",
      "======== Epoch 2 / 100 ========\n",
      "\n",
      "  Average training loss: 5.890\n",
      "  Average training Accuracy: 0.137\n",
      "Validation Accuracy: 0.21\n",
      "  Validation Loss: 5.79\n",
      "======== Epoch 3 / 100 ========\n",
      "\n",
      "  Average training loss: 5.474\n",
      "  Average training Accuracy: 0.265\n",
      "Validation Accuracy: 0.32\n",
      "  Validation Loss: 5.43\n",
      "======== Epoch 4 / 100 ========\n",
      "\n",
      "  Average training loss: 5.092\n",
      "  Average training Accuracy: 0.368\n",
      "Validation Accuracy: 0.36\n",
      "  Validation Loss: 5.11\n",
      "======== Epoch 5 / 100 ========\n",
      "\n",
      "  Average training loss: 4.760\n",
      "  Average training Accuracy: 0.424\n",
      "Validation Accuracy: 0.42\n",
      "  Validation Loss: 4.85\n",
      "======== Epoch 6 / 100 ========\n",
      "\n",
      "  Average training loss: 4.439\n",
      "  Average training Accuracy: 0.467\n",
      "Validation Accuracy: 0.43\n",
      "  Validation Loss: 4.58\n",
      "======== Epoch 7 / 100 ========\n",
      "\n",
      "  Average training loss: 4.155\n",
      "  Average training Accuracy: 0.504\n",
      "Validation Accuracy: 0.45\n",
      "  Validation Loss: 4.36\n",
      "======== Epoch 8 / 100 ========\n",
      "\n",
      "  Average training loss: 3.886\n",
      "  Average training Accuracy: 0.544\n",
      "Validation Accuracy: 0.48\n",
      "  Validation Loss: 4.10\n",
      "======== Epoch 9 / 100 ========\n",
      "\n",
      "  Average training loss: 3.632\n",
      "  Average training Accuracy: 0.563\n",
      "Validation Accuracy: 0.50\n",
      "  Validation Loss: 3.89\n",
      "======== Epoch 10 / 100 ========\n",
      "\n",
      "  Average training loss: 3.391\n",
      "  Average training Accuracy: 0.605\n",
      "Validation Accuracy: 0.52\n",
      "  Validation Loss: 3.70\n",
      "======== Epoch 11 / 100 ========\n",
      "\n",
      "  Average training loss: 3.171\n",
      "  Average training Accuracy: 0.634\n",
      "Validation Accuracy: 0.53\n",
      "  Validation Loss: 3.48\n",
      "======== Epoch 12 / 100 ========\n",
      "\n",
      "  Average training loss: 2.959\n",
      "  Average training Accuracy: 0.663\n",
      "Validation Accuracy: 0.57\n",
      "  Validation Loss: 3.30\n",
      "======== Epoch 13 / 100 ========\n",
      "\n",
      "  Average training loss: 2.773\n",
      "  Average training Accuracy: 0.688\n",
      "Validation Accuracy: 0.57\n",
      "  Validation Loss: 3.19\n",
      "======== Epoch 14 / 100 ========\n",
      "\n",
      "  Average training loss: 2.595\n",
      "  Average training Accuracy: 0.729\n",
      "Validation Accuracy: 0.61\n",
      "  Validation Loss: 3.02\n",
      "======== Epoch 15 / 100 ========\n",
      "\n",
      "  Average training loss: 2.432\n",
      "  Average training Accuracy: 0.749\n",
      "Validation Accuracy: 0.62\n",
      "  Validation Loss: 2.92\n",
      "======== Epoch 16 / 100 ========\n",
      "\n",
      "  Average training loss: 2.276\n",
      "  Average training Accuracy: 0.784\n",
      "Validation Accuracy: 0.67\n",
      "  Validation Loss: 2.73\n",
      "======== Epoch 17 / 100 ========\n",
      "\n",
      "  Average training loss: 2.127\n",
      "  Average training Accuracy: 0.811\n",
      "Validation Accuracy: 0.66\n",
      "  Validation Loss: 2.63\n",
      "======== Epoch 18 / 100 ========\n",
      "\n",
      "  Average training loss: 1.999\n",
      "  Average training Accuracy: 0.830\n",
      "Validation Accuracy: 0.69\n",
      "  Validation Loss: 2.48\n",
      "======== Epoch 19 / 100 ========\n",
      "\n",
      "  Average training loss: 1.863\n",
      "  Average training Accuracy: 0.856\n",
      "Validation Accuracy: 0.74\n",
      "  Validation Loss: 2.38\n",
      "======== Epoch 20 / 100 ========\n",
      "\n",
      "  Average training loss: 1.747\n",
      "  Average training Accuracy: 0.874\n",
      "Validation Accuracy: 0.77\n",
      "  Validation Loss: 2.27\n",
      "======== Epoch 21 / 100 ========\n",
      "\n",
      "  Average training loss: 1.632\n",
      "  Average training Accuracy: 0.890\n",
      "Validation Accuracy: 0.77\n",
      "  Validation Loss: 2.19\n",
      "======== Epoch 22 / 100 ========\n",
      "\n",
      "  Average training loss: 1.527\n",
      "  Average training Accuracy: 0.904\n",
      "Validation Accuracy: 0.78\n",
      "  Validation Loss: 2.09\n",
      "======== Epoch 23 / 100 ========\n",
      "\n",
      "  Average training loss: 1.424\n",
      "  Average training Accuracy: 0.918\n",
      "Validation Accuracy: 0.78\n",
      "  Validation Loss: 2.03\n",
      "======== Epoch 24 / 100 ========\n",
      "\n",
      "  Average training loss: 1.334\n",
      "  Average training Accuracy: 0.927\n",
      "Validation Accuracy: 0.80\n",
      "  Validation Loss: 1.99\n",
      "======== Epoch 25 / 100 ========\n",
      "\n",
      "  Average training loss: 1.243\n",
      "  Average training Accuracy: 0.929\n",
      "Validation Accuracy: 0.81\n",
      "  Validation Loss: 1.93\n",
      "======== Epoch 26 / 100 ========\n",
      "\n",
      "  Average training loss: 1.165\n",
      "  Average training Accuracy: 0.936\n",
      "Validation Accuracy: 0.82\n",
      "  Validation Loss: 1.89\n",
      "======== Epoch 27 / 100 ========\n",
      "\n",
      "  Average training loss: 1.083\n",
      "  Average training Accuracy: 0.945\n",
      "Validation Accuracy: 0.82\n",
      "  Validation Loss: 1.72\n",
      "======== Epoch 28 / 100 ========\n",
      "\n",
      "  Average training loss: 1.013\n",
      "  Average training Accuracy: 0.944\n",
      "Validation Accuracy: 0.85\n",
      "  Validation Loss: 1.68\n",
      "======== Epoch 29 / 100 ========\n",
      "\n",
      "  Average training loss: 0.944\n",
      "  Average training Accuracy: 0.948\n",
      "Validation Accuracy: 0.84\n",
      "  Validation Loss: 1.68\n",
      "======== Epoch 30 / 100 ========\n",
      "\n",
      "  Average training loss: 0.878\n",
      "  Average training Accuracy: 0.948\n",
      "Validation Accuracy: 0.85\n",
      "  Validation Loss: 1.58\n",
      "======== Epoch 31 / 100 ========\n",
      "\n",
      "  Average training loss: 0.817\n",
      "  Average training Accuracy: 0.951\n",
      "Validation Accuracy: 0.85\n",
      "  Validation Loss: 1.47\n",
      "======== Epoch 32 / 100 ========\n",
      "\n",
      "  Average training loss: 0.758\n",
      "  Average training Accuracy: 0.952\n",
      "Validation Accuracy: 0.86\n",
      "  Validation Loss: 1.46\n",
      "======== Epoch 33 / 100 ========\n",
      "\n",
      "  Average training loss: 0.710\n",
      "  Average training Accuracy: 0.950\n",
      "Validation Accuracy: 0.84\n",
      "  Validation Loss: 1.49\n",
      "======== Epoch 34 / 100 ========\n",
      "\n",
      "  Average training loss: 0.655\n",
      "  Average training Accuracy: 0.955\n",
      "Validation Accuracy: 0.85\n",
      "  Validation Loss: 1.38\n",
      "======== Epoch 35 / 100 ========\n",
      "\n",
      "  Average training loss: 0.608\n",
      "  Average training Accuracy: 0.956\n",
      "Validation Accuracy: 0.85\n",
      "  Validation Loss: 1.34\n",
      "======== Epoch 36 / 100 ========\n",
      "\n",
      "  Average training loss: 0.568\n",
      "  Average training Accuracy: 0.955\n",
      "Validation Accuracy: 0.87\n",
      "  Validation Loss: 1.24\n",
      "======== Epoch 37 / 100 ========\n",
      "\n",
      "  Average training loss: 0.529\n",
      "  Average training Accuracy: 0.957\n",
      "Validation Accuracy: 0.88\n",
      "  Validation Loss: 1.20\n",
      "======== Epoch 38 / 100 ========\n",
      "\n",
      "  Average training loss: 0.488\n",
      "  Average training Accuracy: 0.957\n",
      "Validation Accuracy: 0.88\n",
      "  Validation Loss: 1.18\n",
      "======== Epoch 39 / 100 ========\n",
      "\n",
      "  Average training loss: 0.459\n",
      "  Average training Accuracy: 0.959\n",
      "Validation Accuracy: 0.87\n",
      "  Validation Loss: 1.19\n",
      "======== Epoch 40 / 100 ========\n",
      "\n",
      "  Average training loss: 0.422\n",
      "  Average training Accuracy: 0.960\n",
      "Validation Accuracy: 0.88\n",
      "  Validation Loss: 1.15\n",
      "======== Epoch 41 / 100 ========\n",
      "\n",
      "  Average training loss: 0.393\n",
      "  Average training Accuracy: 0.962\n",
      "Validation Accuracy: 0.88\n",
      "  Validation Loss: 1.08\n",
      "======== Epoch 42 / 100 ========\n",
      "\n",
      "  Average training loss: 0.365\n",
      "  Average training Accuracy: 0.964\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 1.02\n",
      "======== Epoch 43 / 100 ========\n",
      "\n",
      "  Average training loss: 0.339\n",
      "  Average training Accuracy: 0.963\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 1.03\n",
      "======== Epoch 44 / 100 ========\n",
      "\n",
      "  Average training loss: 0.316\n",
      "  Average training Accuracy: 0.964\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.96\n",
      "======== Epoch 45 / 100 ========\n",
      "\n",
      "  Average training loss: 0.296\n",
      "  Average training Accuracy: 0.964\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.94\n",
      "======== Epoch 46 / 100 ========\n",
      "\n",
      "  Average training loss: 0.278\n",
      "  Average training Accuracy: 0.964\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.91\n",
      "======== Epoch 47 / 100 ========\n",
      "\n",
      "  Average training loss: 0.257\n",
      "  Average training Accuracy: 0.966\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.92\n",
      "======== Epoch 48 / 100 ========\n",
      "\n",
      "  Average training loss: 0.239\n",
      "  Average training Accuracy: 0.969\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.94\n",
      "======== Epoch 49 / 100 ========\n",
      "\n",
      "  Average training loss: 0.224\n",
      "  Average training Accuracy: 0.968\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.86\n",
      "======== Epoch 50 / 100 ========\n",
      "\n",
      "  Average training loss: 0.214\n",
      "  Average training Accuracy: 0.968\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.84\n",
      "======== Epoch 51 / 100 ========\n",
      "\n",
      "  Average training loss: 0.199\n",
      "  Average training Accuracy: 0.969\n",
      "Validation Accuracy: 0.91\n",
      "  Validation Loss: 0.84\n",
      "======== Epoch 52 / 100 ========\n",
      "\n",
      "  Average training loss: 0.189\n",
      "  Average training Accuracy: 0.970\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.81\n",
      "======== Epoch 53 / 100 ========\n",
      "\n",
      "  Average training loss: 0.176\n",
      "  Average training Accuracy: 0.970\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.81\n",
      "======== Epoch 54 / 100 ========\n",
      "\n",
      "  Average training loss: 0.169\n",
      "  Average training Accuracy: 0.970\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.75\n",
      "======== Epoch 55 / 100 ========\n",
      "\n",
      "  Average training loss: 0.159\n",
      "  Average training Accuracy: 0.969\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.75\n",
      "======== Epoch 56 / 100 ========\n",
      "\n",
      "  Average training loss: 0.152\n",
      "  Average training Accuracy: 0.970\n",
      "Validation Accuracy: 0.91\n",
      "  Validation Loss: 0.73\n",
      "======== Epoch 57 / 100 ========\n",
      "\n",
      "  Average training loss: 0.144\n",
      "  Average training Accuracy: 0.973\n",
      "Validation Accuracy: 0.91\n",
      "  Validation Loss: 0.70\n",
      "======== Epoch 58 / 100 ========\n",
      "\n",
      "  Average training loss: 0.137\n",
      "  Average training Accuracy: 0.972\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.73\n",
      "======== Epoch 59 / 100 ========\n",
      "\n",
      "  Average training loss: 0.132\n",
      "  Average training Accuracy: 0.972\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.72\n",
      "======== Epoch 60 / 100 ========\n",
      "\n",
      "  Average training loss: 0.127\n",
      "  Average training Accuracy: 0.973\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.68\n",
      "======== Epoch 61 / 100 ========\n",
      "\n",
      "  Average training loss: 0.122\n",
      "  Average training Accuracy: 0.973\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.74\n",
      "======== Epoch 62 / 100 ========\n",
      "\n",
      "  Average training loss: 0.117\n",
      "  Average training Accuracy: 0.974\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.72\n",
      "======== Epoch 63 / 100 ========\n",
      "\n",
      "  Average training loss: 0.114\n",
      "  Average training Accuracy: 0.973\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.72\n",
      "======== Epoch 64 / 100 ========\n",
      "\n",
      "  Average training loss: 0.110\n",
      "  Average training Accuracy: 0.974\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.69\n",
      "======== Epoch 65 / 100 ========\n",
      "\n",
      "  Average training loss: 0.105\n",
      "  Average training Accuracy: 0.974\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.69\n",
      "======== Epoch 66 / 100 ========\n",
      "\n",
      "  Average training loss: 0.104\n",
      "  Average training Accuracy: 0.973\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.69\n",
      "======== Epoch 67 / 100 ========\n",
      "\n",
      "  Average training loss: 0.100\n",
      "  Average training Accuracy: 0.974\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.68\n",
      "======== Epoch 68 / 100 ========\n",
      "\n",
      "  Average training loss: 0.096\n",
      "  Average training Accuracy: 0.975\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.65\n",
      "======== Epoch 69 / 100 ========\n",
      "\n",
      "  Average training loss: 0.093\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.66\n",
      "======== Epoch 70 / 100 ========\n",
      "\n",
      "  Average training loss: 0.092\n",
      "  Average training Accuracy: 0.976\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.71\n",
      "======== Epoch 71 / 100 ========\n",
      "\n",
      "  Average training loss: 0.091\n",
      "  Average training Accuracy: 0.976\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.69\n",
      "======== Epoch 72 / 100 ========\n",
      "\n",
      "  Average training loss: 0.087\n",
      "  Average training Accuracy: 0.975\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.65\n",
      "======== Epoch 73 / 100 ========\n",
      "\n",
      "  Average training loss: 0.086\n",
      "  Average training Accuracy: 0.976\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.66\n",
      "======== Epoch 74 / 100 ========\n",
      "\n",
      "  Average training loss: 0.082\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.66\n",
      "======== Epoch 75 / 100 ========\n",
      "\n",
      "  Average training loss: 0.081\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.65\n",
      "======== Epoch 76 / 100 ========\n",
      "\n",
      "  Average training loss: 0.079\n",
      "  Average training Accuracy: 0.975\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.64\n",
      "======== Epoch 77 / 100 ========\n",
      "\n",
      "  Average training loss: 0.078\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.68\n",
      "======== Epoch 78 / 100 ========\n",
      "\n",
      "  Average training loss: 0.077\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.91\n",
      "  Validation Loss: 0.62\n",
      "======== Epoch 79 / 100 ========\n",
      "\n",
      "  Average training loss: 0.074\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.88\n",
      "  Validation Loss: 0.69\n",
      "======== Epoch 80 / 100 ========\n",
      "\n",
      "  Average training loss: 0.074\n",
      "  Average training Accuracy: 0.978\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.63\n",
      "======== Epoch 81 / 100 ========\n",
      "\n",
      "  Average training loss: 0.073\n",
      "  Average training Accuracy: 0.975\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.66\n",
      "======== Epoch 82 / 100 ========\n",
      "\n",
      "  Average training loss: 0.070\n",
      "  Average training Accuracy: 0.978\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.64\n",
      "======== Epoch 83 / 100 ========\n",
      "\n",
      "  Average training loss: 0.071\n",
      "  Average training Accuracy: 0.975\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.64\n",
      "======== Epoch 84 / 100 ========\n",
      "\n",
      "  Average training loss: 0.070\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.64\n",
      "======== Epoch 85 / 100 ========\n",
      "\n",
      "  Average training loss: 0.068\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.64\n",
      "======== Epoch 86 / 100 ========\n",
      "\n",
      "  Average training loss: 0.068\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.67\n",
      "======== Epoch 87 / 100 ========\n",
      "\n",
      "  Average training loss: 0.068\n",
      "  Average training Accuracy: 0.976\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.64\n",
      "======== Epoch 88 / 100 ========\n",
      "\n",
      "  Average training loss: 0.066\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.68\n",
      "======== Epoch 89 / 100 ========\n",
      "\n",
      "  Average training loss: 0.066\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.69\n",
      "======== Epoch 90 / 100 ========\n",
      "\n",
      "  Average training loss: 0.066\n",
      "  Average training Accuracy: 0.976\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.62\n",
      "======== Epoch 91 / 100 ========\n",
      "\n",
      "  Average training loss: 0.066\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.62\n",
      "======== Epoch 92 / 100 ========\n",
      "\n",
      "  Average training loss: 0.064\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.69\n",
      "======== Epoch 93 / 100 ========\n",
      "\n",
      "  Average training loss: 0.063\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.64\n",
      "======== Epoch 94 / 100 ========\n",
      "\n",
      "  Average training loss: 0.063\n",
      "  Average training Accuracy: 0.976\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.67\n",
      "======== Epoch 95 / 100 ========\n",
      "\n",
      "  Average training loss: 0.063\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.66\n",
      "======== Epoch 96 / 100 ========\n",
      "\n",
      "  Average training loss: 0.063\n",
      "  Average training Accuracy: 0.978\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.68\n",
      "======== Epoch 97 / 100 ========\n",
      "\n",
      "  Average training loss: 0.061\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.74\n",
      "======== Epoch 98 / 100 ========\n",
      "\n",
      "  Average training loss: 0.062\n",
      "  Average training Accuracy: 0.976\n",
      "Validation Accuracy: 0.89\n",
      "  Validation Loss: 0.70\n",
      "======== Epoch 99 / 100 ========\n",
      "\n",
      "  Average training loss: 0.062\n",
      "  Average training Accuracy: 0.977\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.63\n",
      "======== Epoch 100 / 100 ========\n",
      "\n",
      "  Average training loss: 0.060\n",
      "  Average training Accuracy: 0.978\n",
      "Validation Accuracy: 0.90\n",
      "  Validation Loss: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "epochs=100\n",
    "print('Training...')\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    train(Train,Val,model,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=np.concatenate(Validation(Test,model))\n",
    "Publisher=label_encoder.inverse_transform(pred)\n",
    "answer=pd.DataFrame({'Id':range(1,len(Publisher)+1),'Publisher':Publisher})\n",
    "answer=answer.set_index('Id')\n",
    "answer.to_csv('Answertorch.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
