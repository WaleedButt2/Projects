{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, IntervalStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 276\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=12, logging_steps=50, save_strategy=IntervalStrategy.NO,\n",
    "                                  per_device_train_batch_size=15, per_device_eval_batch_size=15, warmup_steps=50,\n",
    "                                 weight_decay=0.01, logging_dir='./logs', fp16=True, deepspeed='./ds_config.json')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\").cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "descriptions = pd.read_csv('data.csv')['text']\n",
    "max_length = max([len(tokenizer.encode(description)) for description in descriptions])\n",
    "print(\"Max length: {}\".format(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='EleutherAI/gpt-neo-125m', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-11 21:37:02,916] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Time to load cpu_adam op: 3.4169466495513916 seconds\n",
      "Time to load utils op: 0.0009012222290039062 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/butt_sahab/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module cpu_adam, skipping build step...\n",
      "Loading extension module cpu_adam...\n",
      "Using /home/butt_sahab/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0 partition count [1] and sizes[(125200128, False)] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/butt_sahab/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load utils op: 0.0033655166625976562 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [720/720 28:00, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.068800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=720, training_loss=0.26534559461805557, metrics={'train_runtime': 1690.7174, 'train_samples_per_second': 6.281, 'train_steps_per_second': 0.426, 'total_flos': 1495369800220672.0, 'train_loss': 0.26534559461805557, 'epoch': 20.0})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = PersonalDataset(descriptions, tokenizer, max_length=max_length)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "Trainer(model=model, args=training_args, train_dataset=train_dataset,\n",
    "       eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer(\"<|startoftext|>\", return_tensors=\"pt\").input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Muqeeet, one of your closest freinds in the university. He had decided to help you set up a girl and went out with you and her to help smooth it along. But as it turns out he was sitting by her and talking/laughing while excluding you. You are sitting in KFC with muqeet and the girl nameed AX2. Both muqeet and AX2 are sitting together. You: I think we should switch seats right muqeet? Muqeet: No, I'm just trying to keep things in order.\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"<|startoftext|> Muqeeet, one of your closest freinds in the university. He had decided to help you set up a girl and went out with you and her to help smooth it along. But as it turns out he was sitting by her and talking/laughing while excluding you. You are sitting in KFC with muqeet and the girl nameed AX2. Both muqeet and AX2 are sitting together. You: I think we should switch seats right muqeet? Muqeet:\"\n",
    "encoded_input = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "sample_output=model.generate(encoded_input, \n",
    "                do_sample=True, \n",
    "                top_k=50,\n",
    "                max_length=300, \n",
    "                top_p=0.5, \n",
    "                temperature=0.7,\n",
    "                num_return_sequences=1)\n",
    "output_text = tokenizer.decode(sample_output[0],skip_special_tokens=True)\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./Flaskapp/uo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./Flaskapp/uo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
